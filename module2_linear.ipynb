{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21bff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "a = [2,3,1]\n",
    "b = [4,-1,5]\n",
    "print(np.dot(a,b))\n",
    "# it's equal to 2*4 + 3*(-1) + 1*5 = 8 + -3 + 5 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d7569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd69ba6a",
   "metadata": {},
   "source": [
    "# Matrix Multiplication in a Single Neural Network Layer\n",
    "\n",
    "In a basic neural network, each layer performs computations using matrix multiplication to transform inputs into outputs. Here's how it works:\n",
    "\n",
    "## Forward Pass in a Single Layer\n",
    "\n",
    "1. **Inputs**: The input to a layer is a vector (or batch of vectors) representing features from the previous layer or raw data.\n",
    "\n",
    "2. **Weights Matrix**: Each layer has a weight matrix `W` where:\n",
    "   - Rows correspond to neurons in the current layer\n",
    "   - Columns correspond to inputs from the previous layer\n",
    "   - Each element `W[i,j]` represents the strength of connection from input `j` to neuron `i`\n",
    "\n",
    "3. **Matrix Multiplication**: The core computation is:\n",
    "   ```\n",
    "   Z = W * X + b\n",
    "   ```\n",
    "   Where:\n",
    "   - `X` is the input vector/matrix\n",
    "   - `W` is the weights matrix\n",
    "   - `b` is the bias vector (added to each neuron)\n",
    "   - `Z` is the pre-activation output\n",
    "\n",
    "4. **Activation Function**: Apply a non-linear activation (like ReLU, sigmoid) to `Z` to get the final output `A`:\n",
    "   ```\n",
    "   A = activation(Z)\n",
    "   ```\n",
    "\n",
    "## Why Matrix Multiplication?\n",
    "\n",
    "- **Efficiency**: Matrix operations can be parallelized and optimized (e.g., using GPUs)\n",
    "- **Scalability**: Handles multiple inputs/outputs simultaneously\n",
    "- **Mathematical Foundation**: Represents linear transformations in vector spaces\n",
    "\n",
    "## Example\n",
    "\n",
    "For a layer with 3 neurons and 2 inputs:\n",
    "- `X` = [x1, x2] (1x2 vector)\n",
    "- `W` = [[w11, w12], [w21, w22], [w31, w32]] (3x2 matrix)\n",
    "- `b` = [b1, b2, b3] (3x1 vector)\n",
    "\n",
    "The output `Z` = [w11*x1 + w12*x2 + b1, w21*x1 + w22*x2 + b2, w31*x1 + w32*x2 + b3]\n",
    "\n",
    "This is computed efficiently as matrix multiplication followed by bias addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1839ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a multiply by b is equal to [[0.38551567 0.01301477 0.11131309 0.01814635]\n",
      " [0.03250888 0.66089691 0.23437734 0.08254718]\n",
      " [0.59035857 0.08523107 0.02597058 0.27811855]\n",
      " [0.70732997 0.28160239 0.24776052 0.19897553]]\n",
      "transpose value of a*b is equal to  [[0.38551567 0.03250888 0.59035857 0.70732997]\n",
      " [0.01301477 0.66089691 0.08523107 0.28160239]\n",
      " [0.11131309 0.23437734 0.02597058 0.24776052]\n",
      " [0.01814635 0.08254718 0.27811855 0.19897553]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(4,4)\n",
    "b = np.random.rand(4,4)\n",
    "print(\"a multiply by b is equal to\", a*b)\n",
    "print(\"transpose value of a*b is equal to \", (a*b).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaaadb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = 2x² + 3xy + y³\n",
      "derivative of f(x,y) with respect to x is 4x + 3y\n",
      "derivative of f(x,y) with respect to y is 3x + 3y²\n"
     ]
    }
   ],
   "source": [
    "print(\"f(x,y) = 2x² + 3xy + y³\")\n",
    "print(\"derivative of f(x,y) with respect to x is 4x + 3y\")\n",
    "print(\"derivative of f(x,y) with respect to y is 3x + 3y²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1684bc7",
   "metadata": {},
   "source": [
    "# Why Do We Need Partial Derivatives in Neural Networks?\n",
    "\n",
    "Partial derivatives are essential in neural networks for training through backpropagation, which optimizes the model's parameters to minimize the loss function. Here's why:\n",
    "\n",
    "## Gradient Descent Optimization\n",
    "\n",
    "Neural networks learn by adjusting weights and biases to reduce the error between predictions and actual outputs. This is done using gradient descent, which requires knowing how much each parameter contributes to the total error.\n",
    "\n",
    "## Role of Partial Derivatives\n",
    "\n",
    "1. **Compute Gradients**: Partial derivatives calculate the rate of change of the loss function with respect to each individual parameter (weight or bias). This forms the gradient vector.\n",
    "\n",
    "2. **Backpropagation**: Starting from the output layer, errors are propagated backwards through the network. At each layer, partial derivatives determine how much each weight contributed to the error.\n",
    "\n",
    "3. **Update Rule**: Parameters are updated using:\n",
    "   ```\n",
    "   θ_new = θ_old - learning_rate * ∂Loss/∂θ\n",
    "   ```\n",
    "   Where `∂Loss/∂θ` is the partial derivative of the loss with respect to parameter θ.\n",
    "\n",
    "## Chain Rule in Action\n",
    "\n",
    "Since neural networks are composed of multiple layers with activation functions, the total derivative involves the chain rule. Partial derivatives allow us to break down complex functions into simpler components:\n",
    "\n",
    "- For a multi-layer network: `Loss = f(g(h(x)))`\n",
    "- Partial derivatives compute: `∂Loss/∂x = ∂Loss/∂f * ∂f/∂g * ∂g/∂h * ∂h/∂x`\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- **Efficiency**: Allows computation of gradients for millions of parameters\n",
    "- **Precision**: Provides exact direction for parameter updates\n",
    "- **Convergence**: Ensures the model learns optimal parameters over time\n",
    "\n",
    "Without partial derivatives, we'd have no systematic way to train neural networks effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7a5aa",
   "metadata": {},
   "source": [
    "# Step-by-Step: How Gradient Descent Updates Weights in Simple Linear Regression\n",
    "\n",
    "Gradient descent optimizes the weights (slope and intercept) in linear regression by iteratively minimizing the mean squared error (MSE) loss. Here's the step-by-step process:\n",
    "\n",
    "## 1. Initialize Parameters\n",
    "\n",
    "Start with initial guesses for the weights:\n",
    "- Slope (m): Often initialized to 0 or a small random value\n",
    "- Intercept (b): Often initialized to 0\n",
    "\n",
    "## 2. Define the Loss Function\n",
    "\n",
    "For linear regression, the loss is Mean Squared Error:\n",
    "```\n",
    "MSE = (1/n) * Σ(y_i - ŷ_i)²\n",
    "```\n",
    "Where:\n",
    "- `y_i` is actual value\n",
    "- `ŷ_i = m*x_i + b` is predicted value\n",
    "- `n` is number of data points\n",
    "\n",
    "## 3. Compute Partial Derivatives (Gradients)\n",
    "\n",
    "Calculate how the loss changes with respect to each parameter:\n",
    "\n",
    "**Derivative w.r.t. slope (m):**\n",
    "```\n",
    "∂MSE/∂m = (-2/n) * Σ(x_i * (y_i - ŷ_i))\n",
    "```\n",
    "\n",
    "**Derivative w.r.t. intercept (b):**\n",
    "```\n",
    "∂MSE/∂b = (-2/n) * Σ(y_i - ŷ_i)\n",
    "```\n",
    "\n",
    "These gradients indicate the direction and magnitude of the steepest ascent.\n",
    "\n",
    "## 4. Update Parameters\n",
    "\n",
    "Update each parameter by moving in the opposite direction of the gradient:\n",
    "```\n",
    "m_new = m_old - learning_rate * ∂MSE/∂m\n",
    "b_new = b_old - learning_rate * ∂MSE/∂b\n",
    "```\n",
    "\n",
    "Where `learning_rate` (α) controls the step size (typically 0.01-0.1).\n",
    "\n",
    "## 5. Iterate Until Convergence\n",
    "\n",
    "Repeat steps 3-4 for multiple epochs until:\n",
    "- The loss stops decreasing significantly\n",
    "- A maximum number of iterations is reached\n",
    "- Gradients become very small\n",
    "\n",
    "## 6. Final Model\n",
    "\n",
    "The final `m` and `b` give the best-fit line: `ŷ = m*x + b`\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- **Learning Rate**: Too high → overshooting minimum; Too low → slow convergence\n",
    "- **Batch vs. Stochastic**: Can compute gradients on full dataset (batch) or single points (stochastic)\n",
    "- **Convergence**: Process continues until parameters stabilize at the optimal values that minimize MSE\n",
    "\n",
    "This iterative approach ensures the model learns the relationship between input `x` and output `y` from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3bb974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: x = 8.000000, f(x) = 64.000000\n",
      "Step 2: x = 6.400000, f(x) = 40.960000\n",
      "Step 3: x = 5.120000, f(x) = 26.214400\n",
      "Step 4: x = 4.096000, f(x) = 16.777216\n",
      "Step 5: x = 3.276800, f(x) = 10.737418\n",
      "Step 6: x = 2.621440, f(x) = 6.871948\n",
      "Step 7: x = 2.097152, f(x) = 4.398047\n",
      "Step 8: x = 1.677722, f(x) = 2.814750\n",
      "Step 9: x = 1.342177, f(x) = 1.801440\n",
      "Step 10: x = 1.073742, f(x) = 1.152922\n",
      "Step 11: x = 0.858993, f(x) = 0.737870\n",
      "Step 12: x = 0.687195, f(x) = 0.472237\n",
      "Step 13: x = 0.549756, f(x) = 0.302231\n",
      "Step 14: x = 0.439805, f(x) = 0.193428\n",
      "Step 15: x = 0.351844, f(x) = 0.123794\n",
      "Step 16: x = 0.281475, f(x) = 0.079228\n",
      "Step 17: x = 0.225180, f(x) = 0.050706\n",
      "Step 18: x = 0.180144, f(x) = 0.032452\n",
      "Step 19: x = 0.144115, f(x) = 0.020769\n",
      "Step 20: x = 0.115292, f(x) = 0.013292\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def df(x):\n",
    "    return 2*x  \n",
    "\n",
    "\n",
    "x= 10.0\n",
    "learning_rate = 0.1\n",
    "steps = 20\n",
    "\n",
    "for i in range(steps):\n",
    "    x= x - learning_rate * df(x)\n",
    "    fx = f(x)\n",
    "    print(f\"Step {i+1}: x = {x:.6f}, f(x) = {fx:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08adc24f",
   "metadata": {},
   "source": [
    "# Why Probability Matters in Machine Learning\n",
    "\n",
    "Probability is fundamental to machine learning, especially in classification tasks where models need to express uncertainty and confidence. Here's why it matters:\n",
    "\n",
    "## Uncertainty Quantification\n",
    "\n",
    "Machine learning models often deal with noisy, incomplete, or ambiguous data. Probability allows models to express:\n",
    "- **Confidence levels**: How sure the model is about a prediction\n",
    "- **Uncertainty**: When the model doesn't know the answer\n",
    "- **Risk assessment**: Potential consequences of wrong predictions\n",
    "\n",
    "## Classification Confidence\n",
    "\n",
    "In classification, probability outputs provide more information than hard predictions:\n",
    "\n",
    "### Example: Medical Diagnosis\n",
    "- **Hard prediction**: \"Patient has disease\" vs \"Patient doesn't have disease\"\n",
    "- **Probabilistic prediction**: \"70% chance of disease, 30% chance of no disease\"\n",
    "\n",
    "The probabilistic approach allows doctors to:\n",
    "- Make informed decisions based on confidence levels\n",
    "- Set appropriate thresholds for different risk tolerances\n",
    "- Combine predictions with other evidence\n",
    "\n",
    "## Decision Making Under Uncertainty\n",
    "\n",
    "Probability enables better decision-making:\n",
    "\n",
    "1. **Threshold Selection**: Choose classification thresholds based on cost-benefit analysis\n",
    "   - High-precision tasks (e.g., spam detection): Low false positive rate\n",
    "   - High-recall tasks (e.g., disease screening): Low false negative rate\n",
    "\n",
    "2. **Expected Value Calculations**: Make decisions based on expected outcomes\n",
    "   ```\n",
    "   Expected_Value = P(success) * value_success + P(failure) * value_failure\n",
    "   ```\n",
    "\n",
    "3. **Bayesian Approaches**: Update beliefs as new evidence arrives\n",
    "   - Prior probability + new data = posterior probability\n",
    "\n",
    "## Model Interpretability\n",
    "\n",
    "Probabilistic outputs make models more interpretable:\n",
    "- **Feature importance**: How much each input contributes to the probability\n",
    "- **Model calibration**: Ensuring predicted probabilities match real frequencies\n",
    "- **Trust and transparency**: Users can see model confidence levels\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "- **Autonomous vehicles**: Probability of pedestrian detection affects braking decisions\n",
    "- **Financial risk**: Probability of loan default influences lending decisions\n",
    "- **Weather forecasting**: Probability distributions for precipitation amounts\n",
    "- **Recommendation systems**: Confidence in user preferences\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "Many ML algorithms are built on probabilistic principles:\n",
    "- **Logistic regression**: Outputs probabilities via sigmoid function\n",
    "- **Neural networks**: Softmax for multi-class probabilities\n",
    "- **Bayesian methods**: Explicit probability distributions over parameters\n",
    "\n",
    "Without probability, models would be limited to binary decisions, missing the rich information needed for robust, real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
