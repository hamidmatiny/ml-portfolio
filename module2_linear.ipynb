{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21bff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "a = [2,3,1]\n",
    "b = [4,-1,5]\n",
    "print(np.dot(a,b))\n",
    "# it's equal to 2*4 + 3*(-1) + 1*5 = 8 + -3 + 5 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d7569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd69ba6a",
   "metadata": {},
   "source": [
    "# Matrix Multiplication in a Single Neural Network Layer\n",
    "\n",
    "In a basic neural network, each layer performs computations using matrix multiplication to transform inputs into outputs. Here's how it works:\n",
    "\n",
    "## Forward Pass in a Single Layer\n",
    "\n",
    "1. **Inputs**: The input to a layer is a vector (or batch of vectors) representing features from the previous layer or raw data.\n",
    "\n",
    "2. **Weights Matrix**: Each layer has a weight matrix `W` where:\n",
    "   - Rows correspond to neurons in the current layer\n",
    "   - Columns correspond to inputs from the previous layer\n",
    "   - Each element `W[i,j]` represents the strength of connection from input `j` to neuron `i`\n",
    "\n",
    "3. **Matrix Multiplication**: The core computation is:\n",
    "   ```\n",
    "   Z = W * X + b\n",
    "   ```\n",
    "   Where:\n",
    "   - `X` is the input vector/matrix\n",
    "   - `W` is the weights matrix\n",
    "   - `b` is the bias vector (added to each neuron)\n",
    "   - `Z` is the pre-activation output\n",
    "\n",
    "4. **Activation Function**: Apply a non-linear activation (like ReLU, sigmoid) to `Z` to get the final output `A`:\n",
    "   ```\n",
    "   A = activation(Z)\n",
    "   ```\n",
    "\n",
    "## Why Matrix Multiplication?\n",
    "\n",
    "- **Efficiency**: Matrix operations can be parallelized and optimized (e.g., using GPUs)\n",
    "- **Scalability**: Handles multiple inputs/outputs simultaneously\n",
    "- **Mathematical Foundation**: Represents linear transformations in vector spaces\n",
    "\n",
    "## Example\n",
    "\n",
    "For a layer with 3 neurons and 2 inputs:\n",
    "- `X` = [x1, x2] (1x2 vector)\n",
    "- `W` = [[w11, w12], [w21, w22], [w31, w32]] (3x2 matrix)\n",
    "- `b` = [b1, b2, b3] (3x1 vector)\n",
    "\n",
    "The output `Z` = [w11*x1 + w12*x2 + b1, w21*x1 + w22*x2 + b2, w31*x1 + w32*x2 + b3]\n",
    "\n",
    "This is computed efficiently as matrix multiplication followed by bias addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1839ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a multiply by b is equal to [[0.38551567 0.01301477 0.11131309 0.01814635]\n",
      " [0.03250888 0.66089691 0.23437734 0.08254718]\n",
      " [0.59035857 0.08523107 0.02597058 0.27811855]\n",
      " [0.70732997 0.28160239 0.24776052 0.19897553]]\n",
      "transpose value of a*b is equal to  [[0.38551567 0.03250888 0.59035857 0.70732997]\n",
      " [0.01301477 0.66089691 0.08523107 0.28160239]\n",
      " [0.11131309 0.23437734 0.02597058 0.24776052]\n",
      " [0.01814635 0.08254718 0.27811855 0.19897553]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(4,4)\n",
    "b = np.random.rand(4,4)\n",
    "print(\"a multiply by b is equal to\", a*b)\n",
    "print(\"transpose value of a*b is equal to \", (a*b).T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
